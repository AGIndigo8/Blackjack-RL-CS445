{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Blackjack\n",
    "\n",
    "## August Garibay\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This project will attempt to explore methods of ML on the game of BlackJack in a simulated environment. Card counting methods are abundant, and many are simplified for human use. While some card counting methods are so computationally strenuous that humans cannot use them, computers can easily perform these tasks. Although the computer counting techniques are more robust, they are extensions of the original idea created for humans.\n",
    "\n",
    "It seems likely that the true utility of a computer is wasted in these classical card counting methods and that a more optimal strategy might be found. This project will explore the use of reinforcement learning to converge on such a stategy.\n",
    "In the subsequent section are the manifest goals of this project. Some of these may not be attainable given the limitations of this project, but I will attempt to achieve as many of them as possible.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "Listed below are my prospective goals for this assignment, roughly in order I intend to execute them. The first are of higher priority, while later objectives will depend on factors such as time and resources. Some of these are just ideas, in this way, and may not be achieved in the scope of this project.\n",
    "\n",
    "* Train networks to understand basic mechanics of hit/stay in a single player environment.\n",
    "* Train networks to play against a (dealer) opponent.\n",
    "* Train networks to play in a group with a dealer, allowing for examination of the networks ability to intuit probability information from the dealings of other players\n",
    "* Encorporate betting by changing the reinforcement function to maximize winnings\n",
    "* Compare performance to classical methods of card counting\n",
    "* Compare performance of networks trained with a bet-driven reinforcement to those that are first rewarded for playing well and later rewarded for betting.\n",
    "* Use auto-encoding to compress information related to card frequency probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackjack\n",
    "\n",
    "Let's start by simulating the game of blackjack so that we have an environment to work in. We will use a dictionary to encode the cards identitites and a stack structure to simulate a deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import copy\n",
    "import neuralnetworks as nn #will be using this given code from A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is needed to make the A6 neuralnetworks code work on my computer\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = {\n",
    "    'Ace': 11,\n",
    "    'King': 10,\n",
    "    'Queen': 10,\n",
    "    'Jack': 10,\n",
    "    '10': 10,\n",
    "    '9': 9,\n",
    "    '8': 8,\n",
    "    '7': 7,\n",
    "    '6': 6,\n",
    "    '5': 5,\n",
    "    '4': 4,\n",
    "    '3': 3,\n",
    "    '2': 2\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "class Shoe:\n",
    "    def __init__(self, num_decks):\n",
    "        self.num_decks = num_decks\n",
    "        self.cards = []\n",
    "        self.discard = []\n",
    "        self.reset()\n",
    "\n",
    "    # Resets the shoe to the original state\n",
    "    # Creates a list of cards and shuffles them\n",
    "    def reset(self):\n",
    "        self.cards = []\n",
    "        for _ in range(self.num_decks*4):\n",
    "            for card in cards:\n",
    "                self.cards.append(card)\n",
    "        random.shuffle(self.cards)\n",
    "\n",
    "    def draw(self):\n",
    "        if self.cards == []:\n",
    "            self.cards = self.discard\n",
    "            self.discard = []\n",
    "            random.shuffle(self.cards)\n",
    "        return self.cards.pop()\n",
    "\n",
    "    def discard(self, card):\n",
    "        self.discard.append(card)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def get_max_size(self):\n",
    "        return self.num_decks*52"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should suffice to simulate the cards. Next we define some useful functions for the game.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bust(hand):\n",
    "    return hand_value(hand) > 21\n",
    "\n",
    "def hit(hand, shoe):\n",
    "    hand.append(shoe.draw())\n",
    "\n",
    "def hand_value(hand):\n",
    "    hand_value = 0\n",
    "    for card in hand:\n",
    "        hand_value += cards[card]\n",
    "    \n",
    "    if hand_value > 21:\n",
    "        for card in hand:\n",
    "            if card == 'Ace':\n",
    "                hand_value -= 10\n",
    "                if hand_value <= 21:\n",
    "                    break\n",
    "    return hand_value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part we will simply ask the Qnet to decide whether to hit or stay. The reward function will either give the value of the agents hand or -1 if the agent busts.\n",
    "For the state vector, we will enumerate the number of each card that the agent has in its hand.\n",
    "It could be preferable, in this context, to simply give the score of the hand and the number of aces.\n",
    "The idea behind giving total information about the hand will be more apparent in later parts of the project, as the goal is to give information about statistical probability of cards remaining in the shoe.\n",
    "\n",
    "This section is really to get the setup working before complicating things with a dealer and other players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = hit and 1 = stay\n",
    "actions = [0, 1]\n",
    "\n",
    "def solo_reinforcement(hand):\n",
    "    if bust(hand):\n",
    "        return -1\n",
    "    return hand_value(hand)\n",
    "\n",
    "def valid_actions(hand):\n",
    "    if bust(hand):\n",
    "        return []\n",
    "    return actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following was taken from assignment 6 and will prove useful throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_sa(s, a):\n",
    "    return np.hstack((s, a)).reshape(1, -1)\n",
    "    \n",
    "#small alteration made to the code given in A6 i.e. the addition of the hand parameter which is used to find valid actions\n",
    "def epsilon_greedy(Qnet, state, epsilon, hand):\n",
    "    \n",
    "    actions = valid_actions(hand)\n",
    "    \n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        action = np.random.choice(actions)\n",
    "        \n",
    "    else:\n",
    "        # Greedy Move\n",
    "        np.random.shuffle(actions)\n",
    "        Qs = np.array([Qnet.use(stack_sa(state, a)) for a in actions])\n",
    "        action = actions[np.argmax(Qs)]\n",
    "        \n",
    "    return action\n",
    "\n",
    "def setup_standardization(Qnet, Xmeans, Xstds, Tmeans, Tstds):\n",
    "    Qnet.Xmeans = np.array(Xmeans)\n",
    "    Qnet.Xstds = np.array(Xstds)\n",
    "    Qnet.Tmeans = np.array(Tmeans)\n",
    "    Qnet.Tstds = np.array(Tstds)\n",
    "    Qnet.Xconstant = Qnet.Xstds == 0\n",
    "    Qnet.Tconstant = Qnet.Tstds == 0\n",
    "    Qnet.XstdsFixed = copy.copy(Qnet.Xstds)\n",
    "    Qnet.TstdsFixed = copy.copy(Qnet.Tstds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function turns the raw hand information into a vector to be used as an input for the Qnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hand_vector(hand):\n",
    "    hand_vector = np.zeros(11)\n",
    "    for card in hand:\n",
    "        if card == 'Ace':\n",
    "            hand_vector[0] += 1\n",
    "        elif card == 'King' or card == 'Queen' or card == 'Jack':\n",
    "            hand_vector[1] += 1\n",
    "        else:\n",
    "            hand_vector[int(card)] += 1\n",
    "    return hand_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play a single hand of blackjack\n",
    "#adapted from the code given in A6 to work with the new reinforcement function\n",
    "def solo_hand(Qnet, shoe, reinforcement_f, epsilon):\n",
    "    Samples = {'SA': [], 'R': [], 'Qnext': []}\n",
    "    \n",
    "    hand = []\n",
    "    hit(hand, shoe)\n",
    "    hit(hand, shoe)\n",
    "    \n",
    "    while True:\n",
    "        state = make_hand_vector(hand)\n",
    "        action = epsilon_greedy(Qnet, state, epsilon, hand)\n",
    "        if action == 1:\n",
    "            reward = reinforcement_f(hand)\n",
    "            Samples['SA'].append(stack_sa(state, action))\n",
    "            Samples['R'].append(reward)\n",
    "            Samples['Qnext'].append(Qnet.use(stack_sa(state, action)))\n",
    "            break\n",
    "        hit(hand, shoe)\n",
    "\n",
    "        reward = reinforcement_f(hand)\n",
    "        Samples['SA'].append(stack_sa(state, action))\n",
    "        Samples['R'].append(reward)\n",
    "        Samples['Qnext'].append(Qnet.use(stack_sa(state, action)))\n",
    "        if bust(hand):\n",
    "            break\n",
    "\n",
    "    Samples['SA'] = np.vstack(Samples['SA'])\n",
    "    Samples['R'] = np.array(Samples['R']).reshape(-1, 1)\n",
    "    Samples['Qnext'] = np.array(Samples['Qnext']).reshape(-1, 1)\n",
    "    return Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we are ready to set-up the agent environment for this simple case. \n",
    "Each round will be given a new shoe, so the agent will not have much probability information to work with.\n",
    "The agent will need to learn basic principles of when to hit and stay based on its own hand.\n",
    "The agent is not anticipated to perform very well for this reason. \n",
    "The agent will be trained for 1000 rounds, and then tested for 1000 rounds.\n",
    "\n",
    "The approach will take inspiration from A6.\n",
    "This however will create a class for encapsulating the agent and its environment so that it can be easily modified for our future cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class solo_play_runner:\n",
    "    def __init__(self, shoe, reinforcement_f, epsilon_final):\n",
    "        self.shoe = shoe\n",
    "        self.reinforcement_f = reinforcement_f\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = np.exp(np.log(self.epsilon_final/self.epsilon))\n",
    "        self.gamma = 1\n",
    "        self.outcomes = []\n",
    "        self.Qnets = []\n",
    "    \n",
    "    ''' \n",
    "    Runs the experiment for nn's of the given parameters and returns the results\n",
    "    This function is the main grid search loop\n",
    "    This method should not be overridden\n",
    "    '''\n",
    "    def run(self, n_hiddens_list, n_epochs_list, learning_rate_list, repetitions = 5, n_trials = 1000):\n",
    "        self.results = []\n",
    "        for n_hiddens in n_hiddens_list:\n",
    "            for n_epoch in n_epochs_list:\n",
    "                for learning_rate in learning_rate_list:\n",
    "                    for rep in range(repetitions):\n",
    "                        self.results.append(self.run_repetition(n_hiddens, n_epoch, learning_rate, n_trials))\n",
    "                        print(self.results[-1])\n",
    "                        self.Qnets.append(self.Qnet)\n",
    "\n",
    "    \"\"\"\n",
    "    The following 3 methods should be overridden by the subclass\n",
    "    This should be sufficient for most experiments\n",
    "    \"\"\"\n",
    "\n",
    "    #info about the input and output vectors\n",
    "    def get_signatures(self):\n",
    "        ace = 1\n",
    "        K_Q_J = 1\n",
    "        num_cards = 9\n",
    "        num_action = 1\n",
    "        #sum of the above is 11\n",
    "        signatures = {\n",
    "            \"input_length\": ace + K_Q_J + num_cards + num_action,\n",
    "            \"output_length\": 1 \n",
    "        }\n",
    "        return signatures\n",
    "    \n",
    "    #converts the arguments to the input vector\n",
    "    #should be overridden by the subclass\n",
    "    #args is a dictionary with any kind of protocol as long as follow it when calling get_input_vector\n",
    "    #Note this is not needed for the solo_play_runner\n",
    "    def get_input_vector(self, args):\n",
    "        hand = args['hand']\n",
    "        s = make_hand_vector(hand)\n",
    "        a = args['action']\n",
    "        return stack_sa(s, a)\n",
    "\n",
    "    # this is the game logic of a trial of the experiment\n",
    "    # this should be overridden by the subclass\n",
    "    # this method should return the result of the trial\n",
    "    def get_samples(self):\n",
    "        return solo_hand(self.Qnet, Shoe(1), self.reinforcement_f, self.epsilon)\n",
    "\n",
    "    \"\"\"\n",
    "    Runs a single repetition of the experiment for the given parameters\n",
    "    This method should not be overridden\n",
    "    \"\"\"\n",
    "    def run_repetition(self, n_hiddens, n_epoch, learning_rate, n_trials):\n",
    "        signatures = self.get_signatures()\n",
    "        self.Qnet = nn.NeuralNetwork(signatures[\"input_length\"], n_hiddens, signatures[\"output_length\"])\n",
    "\n",
    "        setup_standardization(self.Qnet, [0]*signatures[\"input_length\"], [1]*signatures[\"input_length\"], [0]*signatures[\"output_length\"], [1]*signatures[\"output_length\"])\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.outcomes = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            result = self.run_trial(n_epoch, learning_rate)\n",
    "\n",
    "        return [n_hiddens, n_epoch, learning_rate ,np.mean(self.outcomes)]\n",
    "\n",
    "    \"\"\"\n",
    "    Runs a single trial of the experiment for the given parameters\n",
    "    This method should not be overridden\n",
    "    \"\"\"\n",
    "    def run_trial(self, n_epoch, learning_rate):\n",
    "        Samples = self.get_samples()\n",
    "        SA = Samples['SA']\n",
    "        R = Samples['R']\n",
    "        Qn = Samples['Qnext']\n",
    "        T = R + self.gamma * Qn\n",
    "        self.Qnet.train(Samples['SA'], T, n_epoch, method='sgd', learning_rate=learning_rate)\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.outcomes.append(Samples['R'][-1])\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def get_Qnet(self):\n",
    "        return self.Qnet\n",
    "    \n",
    "    def get_Qnets(self):\n",
    "        return self.Qnets\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Single Player Qnets\n",
    "We will now try this simple agent/ environment  setup with a few architectures to see how it performs.\n",
    "Then we will play a few hands with the best performing Qnet to see how it performs.\n",
    "\n",
    "To interpret the ouput of the runner, look to the final value, which is the average reinforcement on some of the last hands played.\n",
    "Since we will be reusing the runner class type throughout the project, this interpretation approach will persist.\n",
    "\n",
    "For this particular case, the reinforcement is the final vaue of the hand. We would like to see this as close to 21 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 50, 10], 100, 0.1, 11.057375]\n",
      "[[10, 50, 10], 100, 0.01, 14.005375]\n",
      "[[10, 50, 10], 50, 0.1, 8.339]\n",
      "[[10, 50, 10], 50, 0.01, 14.49375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomga\\OneDrive\\Documents\\College\\August\\Spring 2023\\cs445\\Final\\neuralnetworks.py:123: RuntimeWarning: overflow encountered in square\n",
      "  return 0.5 * self.mean((T - Y)**2) #Loss Function = Mean Square Error\n",
      "c:\\Users\\tomga\\OneDrive\\Documents\\College\\August\\Spring 2023\\cs445\\Final\\neuralnetworks.py:134: RuntimeWarning: overflow encountered in matmul\n",
      "  delta = (1 - Z[-1]**2) * (delta @ self.W[1:, :].T)\n",
      "c:\\Users\\tomga\\OneDrive\\Documents\\College\\August\\Spring 2023\\cs445\\Final\\neuralnetworks.py:134: RuntimeWarning: invalid value encountered in multiply\n",
      "  delta = (1 - Z[-1]**2) * (delta @ self.W[1:, :].T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20], 100, 0.1, 10.297625]\n",
      "[[20, 20], 100, 0.01, 14.5885]\n",
      "[[20, 20], 50, 0.1, 10.17525]\n",
      "[[20, 20], 50, 0.01, 14.54275]\n"
     ]
    }
   ],
   "source": [
    "runner = solo_play_runner(Shoe(1), solo_reinforcement, 0.1)\n",
    "runner.run([[10,50,10], [20, 20]], \n",
    "           [100, 50], \n",
    "           [0.1, .01],\n",
    "            1, 8000\n",
    "           )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the `.01` learning rate outperforms the `.1`. There is very little difference among the different experiments that share the `.01` learning rate in common.\n",
    "Based on the other results, we will choose an architecture and try to improve this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20], 100, 0.01, 14.500625]\n",
      "[[20, 20], 100, 0.001, 13.991875]\n",
      "[[20, 20], 200, 0.01, 14.624]\n",
      "[[20, 20], 200, 0.001, 14.572875]\n"
     ]
    }
   ],
   "source": [
    "runner = solo_play_runner(Shoe(1), solo_reinforcement, 0.1)\n",
    "runner.run([[20, 20]],\n",
    "           [100, 200],\n",
    "           [.01, .001],\n",
    "            1, 8000\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20], 200, 0.01, 14.479]\n"
     ]
    }
   ],
   "source": [
    "#Best performer\n",
    "runner = solo_play_runner(Shoe(1), solo_reinforcement, 0.1)\n",
    "runner.run([[20, 20]],[200], [.01], 1, 8000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating this performance, it looks like the Qnet is not inclined to bust very often, or the expected hand value would be lower.\n",
    "While promising, this may leave a bit to be desired, and it isn't clear yet how they decisions are being made."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a few hands solo with the best Qnet\n",
    "\n",
    "We will simulate a few hands with the best performing Qnet to see how it performs.\n",
    "We will state the events of the game in plain english print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the game as a narrative\n",
    "def play_solo(Qnet):\n",
    "    hand = []\n",
    "    shoe = Shoe(1)\n",
    "    hit(hand, shoe)\n",
    "    hit(hand, shoe)\n",
    "    print(\"The agent starts with the hand: \", hand)\n",
    "    while True:\n",
    "        state = make_hand_vector(hand)\n",
    "        action = epsilon_greedy(Qnet, state, 0, hand)\n",
    "        if action == 1:\n",
    "            print('stand')\n",
    "            break\n",
    "        hit(hand, shoe)\n",
    "        print('hit', hand)\n",
    "        if bust(hand):\n",
    "            print('bust')\n",
    "            break\n",
    "    print('Final score: ', hand_value(hand))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent starts with the hand:  ['Ace', '10']\n",
      "stand\n",
      "Final score:  21\n",
      "\n",
      "The agent starts with the hand:  ['Ace', 'Jack']\n",
      "stand\n",
      "Final score:  21\n",
      "\n",
      "The agent starts with the hand:  ['7', '9']\n",
      "stand\n",
      "Final score:  16\n",
      "\n",
      "The agent starts with the hand:  ['4', 'Ace']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['3', '9']\n",
      "stand\n",
      "Final score:  12\n",
      "\n",
      "The agent starts with the hand:  ['8', '9']\n",
      "stand\n",
      "Final score:  17\n",
      "\n",
      "The agent starts with the hand:  ['8', 'Queen']\n",
      "stand\n",
      "Final score:  18\n",
      "\n",
      "The agent starts with the hand:  ['Jack', '10']\n",
      "stand\n",
      "Final score:  20\n",
      "\n",
      "The agent starts with the hand:  ['9', '6']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['9', '2']\n",
      "stand\n",
      "Final score:  11\n",
      "\n",
      "The agent starts with the hand:  ['Jack', '5']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['6', '2']\n",
      "stand\n",
      "Final score:  8\n",
      "\n",
      "The agent starts with the hand:  ['King', '7']\n",
      "stand\n",
      "Final score:  17\n",
      "\n",
      "The agent starts with the hand:  ['9', '2']\n",
      "stand\n",
      "Final score:  11\n",
      "\n",
      "The agent starts with the hand:  ['King', '6']\n",
      "stand\n",
      "Final score:  16\n",
      "\n",
      "The agent starts with the hand:  ['10', 'Ace']\n",
      "stand\n",
      "Final score:  21\n",
      "\n",
      "The agent starts with the hand:  ['Queen', 'King']\n",
      "stand\n",
      "Final score:  20\n",
      "\n",
      "The agent starts with the hand:  ['4', 'Queen']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['10', '5']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['3', '3']\n",
      "stand\n",
      "Final score:  6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    np.random.seed(i)\n",
    "    play_solo(runner.get_Qnet())\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the best performing agent simply learns to stand in any situation.\n",
    "This is not the best strategy, but it is reasonable for the Qnet to arrive at this solution.\n",
    "Earlier experimentation that is not included above laned on this simple achitecture that plays in a much more natural way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10], 100, 0.1, 13.157]\n"
     ]
    }
   ],
   "source": [
    "runner = solo_play_runner(Shoe(1), solo_reinforcement, 0.1)\n",
    "runner.run([[10]],[100], [.1], 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent starts with the hand:  ['10', 'Ace']\n",
      "hit ['10', 'Ace', '4']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['8', 'Queen']\n",
      "stand\n",
      "Final score:  18\n",
      "\n",
      "The agent starts with the hand:  ['8', 'King']\n",
      "stand\n",
      "Final score:  18\n",
      "\n",
      "The agent starts with the hand:  ['Queen', '7']\n",
      "stand\n",
      "Final score:  17\n",
      "\n",
      "The agent starts with the hand:  ['5', '2']\n",
      "stand\n",
      "Final score:  7\n",
      "\n",
      "The agent starts with the hand:  ['5', 'King']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['2', '4']\n",
      "stand\n",
      "Final score:  6\n",
      "\n",
      "The agent starts with the hand:  ['Jack', '7']\n",
      "stand\n",
      "Final score:  17\n",
      "\n",
      "The agent starts with the hand:  ['King', '6']\n",
      "stand\n",
      "Final score:  16\n",
      "\n",
      "The agent starts with the hand:  ['9', '6']\n",
      "hit ['9', '6', '2']\n",
      "hit ['9', '6', '2', 'Queen']\n",
      "bust\n",
      "Final score:  27\n",
      "\n",
      "The agent starts with the hand:  ['Queen', 'Ace']\n",
      "hit ['Queen', 'Ace', 'Jack']\n",
      "stand\n",
      "Final score:  21\n",
      "\n",
      "The agent starts with the hand:  ['6', '2']\n",
      "hit ['6', '2', '5']\n",
      "stand\n",
      "Final score:  13\n",
      "\n",
      "The agent starts with the hand:  ['4', '10']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['2', 'Queen']\n",
      "stand\n",
      "Final score:  12\n",
      "\n",
      "The agent starts with the hand:  ['2', '4']\n",
      "stand\n",
      "Final score:  6\n",
      "\n",
      "The agent starts with the hand:  ['4', '4']\n",
      "stand\n",
      "Final score:  8\n",
      "\n",
      "The agent starts with the hand:  ['8', '3']\n",
      "stand\n",
      "Final score:  11\n",
      "\n",
      "The agent starts with the hand:  ['3', 'Ace']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['3', '2']\n",
      "hit ['3', '2', 'King']\n",
      "stand\n",
      "Final score:  15\n",
      "\n",
      "The agent starts with the hand:  ['King', '3']\n",
      "stand\n",
      "Final score:  13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    np.random.seed(i)\n",
    "    play_solo(runner.get_Qnet())\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the issue of playing too conservatively we will modify the reward function.\n",
    "Without recognizing the competition, it is reasonable that the Qnet learns to be conservative, since most points are free and many are lost in a bust.\n",
    "To fix this I will try transforming the hand value with a non-linear growth function.\n",
    "This will give the Qnet more incentive to play for higher scores.\n",
    "\n",
    "By using an exponential filter we gaurentee that the reward for a single extra point outways the pentalty for losing all points.\n",
    "Also by treating busting slightly ddifferently we can give it a little extra fear of just flat our losing.\n",
    "Hopefully this will encourage riskier behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_solo_reward(hand):\n",
    "    hand_val = hand_value(hand)\n",
    "    if bust(hand):\n",
    "        hand_val = -.5\n",
    "    abs_val = np.abs(hand_val)\n",
    "    hand_sign = np.sign(hand_val)\n",
    "    hand_val = np.exp(abs_val) * hand_sign\n",
    "    return hand_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result(runner):\n",
    "    raw = runner.get_results()[-1][-1]\n",
    "    return np.log(np.abs(raw)) * np.sign(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 20], 200, 0.01, 124574388.82746418]\n",
      "18.640413596058814\n"
     ]
    }
   ],
   "source": [
    "runner = solo_play_runner(Shoe(1), nonlinear_solo_reward, 0.1)\n",
    "runner.run([[20, 20]],[200], [.01], 1, 8000)\n",
    "print(convert_result(runner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04695892333984375\n",
      "The agent starts with the hand:  ['King', 'Jack']\n",
      "stand\n",
      "Final score:  20\n",
      "\n",
      "The agent starts with the hand:  ['10', '9']\n",
      "hit ['10', '9', 'King']\n",
      "bust\n",
      "Final score:  29\n",
      "\n",
      "The agent starts with the hand:  ['9', '3']\n",
      "hit ['9', '3', '6']\n",
      "hit ['9', '3', '6', '2']\n",
      "stand\n",
      "Final score:  20\n",
      "\n",
      "The agent starts with the hand:  ['4', '9']\n",
      "stand\n",
      "Final score:  13\n",
      "\n",
      "The agent starts with the hand:  ['2', '6']\n",
      "stand\n",
      "Final score:  8\n",
      "\n",
      "The agent starts with the hand:  ['8', '6']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['2', 'Jack']\n",
      "stand\n",
      "Final score:  12\n",
      "\n",
      "The agent starts with the hand:  ['9', '8']\n",
      "stand\n",
      "Final score:  17\n",
      "\n",
      "The agent starts with the hand:  ['10', 'Ace']\n",
      "stand\n",
      "Final score:  21\n",
      "\n",
      "The agent starts with the hand:  ['Jack', '6']\n",
      "hit ['Jack', '6', '6']\n",
      "bust\n",
      "Final score:  22\n",
      "\n",
      "The agent starts with the hand:  ['8', '10']\n",
      "stand\n",
      "Final score:  18\n",
      "\n",
      "The agent starts with the hand:  ['Jack', '2']\n",
      "hit ['Jack', '2', '4']\n",
      "hit ['Jack', '2', '4', '9']\n",
      "bust\n",
      "Final score:  25\n",
      "\n",
      "The agent starts with the hand:  ['4', '10']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['King', '5']\n",
      "hit ['King', '5', '10']\n",
      "bust\n",
      "Final score:  25\n",
      "\n",
      "The agent starts with the hand:  ['9', '5']\n",
      "stand\n",
      "Final score:  14\n",
      "\n",
      "The agent starts with the hand:  ['5', '9']\n",
      "hit ['5', '9', '10']\n",
      "bust\n",
      "Final score:  24\n",
      "\n",
      "The agent starts with the hand:  ['2', 'Jack']\n",
      "hit ['2', 'Jack', '9']\n",
      "hit ['2', 'Jack', '9', 'Jack']\n",
      "bust\n",
      "Final score:  31\n",
      "\n",
      "The agent starts with the hand:  ['5', '2']\n",
      "hit ['5', '2', 'King']\n",
      "hit ['5', '2', 'King', '6']\n",
      "bust\n",
      "Final score:  23\n",
      "\n",
      "The agent starts with the hand:  ['4', '7']\n",
      "stand\n",
      "Final score:  11\n",
      "\n",
      "The agent starts with the hand:  ['2', '7']\n",
      "stand\n",
      "Final score:  9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qnet = runner.get_Qnet()\n",
    "print(qnet.get_training_time())\n",
    "for i in range(20):\n",
    "    np.random.seed(i)\n",
    "    play_solo(runner.get_Qnet())\n",
    "    print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is markedly better than the previous one.\n",
    "It still makes dubious decisions, but it is more versitile in its play.\n",
    "Further the average hand value is much higher than the previous one.\n",
    "These strategies will need to be adjusted however for when we play for our other cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casino Blackjack with a Dealer and other Players but no Betting\n",
    "\n",
    "Now we will add a dealer and other players to the game.\n",
    "The approach for this is as follows:\n",
    "\n",
    "* The `agent hand` is handled as before\n",
    "* The `dealer hand` is treated as a seperate hand\n",
    "* The `other players hand` (including the dealer) are treated as a single hand i.e. this hand is all non agent cards dealt.\n",
    "\n",
    "For the Qnet the state vector will be much larger then before to keep track of all this information.\n",
    "The agent hand will be vectorized and stacked with the hand vector for the other players hand.\n",
    "The dealer hand will be turned into a simpler vector before stacking, which contains only the visible card score and number of aces.\n",
    "Finally we provide a single element to tell the agent the number of decks in the shoe.\n",
    "\n",
    "This is *27* dimensional state vector, which is much larger than the previous *12* dimensional vector.\n",
    "\n",
    "The objective here is simply for the agent to know when to hit and stay with better confidence from the extra probability information and win more often.\n",
    "Winning in this context is redefined to be beating the dealers hand without busting.\n",
    "\n",
    "The dealer will play by the following rules:\n",
    "\n",
    "* If the dealer has a score of 17 or more, they will stay\n",
    "* If the dealer has a score of 16 or less, they will hit\n",
    "\n",
    "The other players will be dealt cards from the same shoe as everyone else, but we don't care about their hands.\n",
    "So we will play a card to the `other players hand` for each player and simply let them play by the same rules as the dealer.\n",
    "\n",
    "We will carry on over several hands like this until the shoe gets too low on cards.\n",
    "After each hand the information about the `other players hand` will be kept as an intial state for the next hand.\n",
    "the `other players hand` will reset when the shoe does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some useful functions for this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_dealer_hit(hand):\n",
    "    return hand_value(hand) < 17\n",
    "\n",
    "#When the dealer hits we need to add the card to the `other_hand` list too so that we can keep track of the contents of the shoe.\n",
    "def hit_dealer(dealer_hand, other_hand, shoe):\n",
    "    hit(dealer_hand, shoe)\n",
    "    other_hand.append(dealer_hand[-1])\n",
    "\n",
    "def dealers_play(dealer_hand, other_hand, shoe):\n",
    "    while does_dealer_hit(dealer_hand):\n",
    "        hit_dealer(dealer_hand, other_hand, shoe)\n",
    "\n",
    "def other_players_play(other_hand, shoe):\n",
    "    while True:\n",
    "        if hand_value(other_hand) >= 17:\n",
    "            break\n",
    "        hit(other_hand, shoe)\n",
    "\n",
    "def deal(shoe, player_hand, other_hand, dealer_hand, n_others):\n",
    "    for _ in range(2):\n",
    "        hit(player_hand, shoe)\n",
    "        for _ in range(n_others):\n",
    "            hit(other_hand, shoe)\n",
    "        hit(dealer_hand, shoe)\n",
    "\n",
    "def reinforcement_group(hand, dealer_hand, end):\n",
    "    if bust(hand):\n",
    "        return -1\n",
    "    if bust(dealer_hand):\n",
    "        return 1\n",
    "    if hand_value(hand) > hand_value(dealer_hand) and end:\n",
    "        return 1\n",
    "    if hand_value(hand) < hand_value(dealer_hand) and end:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def is_shoe_done(shoe):\n",
    "    return len(shoe) < shoe.get_max_size() * 0.25\n",
    "\n",
    "def get_num_of_other_players(shoe):\n",
    "    return shoe.get_max_size() // 52 - 1\n",
    "\n",
    "def make_dealer_hand_vector(hand):\n",
    "    visible_part = hand[1:]\n",
    "    vis_vector = make_hand_vector(visible_part)\n",
    "    vis_value = hand_value(visible_part)\n",
    "    num_aces_visible = vis_vector[0]\n",
    "    result = np.array([num_aces_visible, vis_value])\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the experiment runner, which will inherit from the previous with a few modifications.\n",
    "The new runner must reflect the new input space in the `get_signatures` method and change the way the input state vector is initialized.\n",
    "The `get_samples` method will now need to reflect the new rules for the game and gameplay flow.\n",
    "These exact changes will be pervasive throught the project, so I will not comment on them in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class group_play_runner(solo_play_runner):\n",
    "    def __init__(self, shoe, reinforcement_f, epsilon_final):\n",
    "        super().__init__(shoe, reinforcement_f, epsilon_final)\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        self.other_hands = []\n",
    "\n",
    "    def get_signatures(self):\n",
    "        hand_vector_size = 11\n",
    "        dealer_hand_vector_size = 2\n",
    "        shoe_information = 1 \n",
    "        num_actions = 1 #stand or hit is only 1 type of action\n",
    "        return {\n",
    "            \"input_length\": hand_vector_size*2 + dealer_hand_vector_size + shoe_information + num_actions,\n",
    "            \"output_length\": 1\n",
    "        }\n",
    "    \n",
    "    def get_input_vector(self, args):\n",
    "        hand = args['hand']\n",
    "        dealer_hand = args['dealer_hand']\n",
    "        shoe = args['shoe']\n",
    "        others_hand = args['others']\n",
    "        player = make_hand_vector(hand)\n",
    "        others =make_hand_vector(others_hand)\n",
    "        dealer = make_dealer_hand_vector(dealer_hand)\n",
    "        out =np.hstack([player, others, dealer, shoe.get_max_size() - len(shoe)])\n",
    "        return out\n",
    "    \n",
    "    def get_samples(self):\n",
    "        Samples = { \"SA\": [], \"R\": [], \"Qnext\": [] }\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "\n",
    "        n_others = get_num_of_other_players(self.shoe)\n",
    "        if is_shoe_done(self.shoe):\n",
    "            #number of decks between 1 and 6\n",
    "            self.shoe = Shoe(np.random.randint(1, 7))\n",
    "            self.other_hands = []\n",
    "\n",
    "        deal(self.shoe, self.player_hand, self.other_hands, self.dealer_hand, n_others)\n",
    "\n",
    "        for _ in range(n_others):\n",
    "            other_players_play(self.other_hands, self.shoe)\n",
    "\n",
    "        while True:\n",
    "            state = self.get_input_vector({\n",
    "                \"hand\": self.player_hand,\n",
    "                \"others\": self.other_hands,\n",
    "                \"dealer_hand\": self.dealer_hand,\n",
    "                \"shoe\": self.shoe,\n",
    "            })\n",
    "            action = epsilon_greedy(self.Qnet, state, self.epsilon, self.player_hand)\n",
    "            \n",
    "            if action == 1:\n",
    "                reward = self.reinforcement_f(self.player_hand, self.dealer_hand, False)\n",
    "                Samples['SA'].append(stack_sa(state, action))\n",
    "                Samples['R'].append(reward)\n",
    "                Samples['Qnext'].append(self.Qnet.use(stack_sa(state, action)))\n",
    "                break\n",
    "        \n",
    "            hit(self.player_hand, self.shoe)\n",
    "\n",
    "            reward = self.reinforcement_f(self.player_hand, self.dealer_hand, False)\n",
    "            Samples['SA'].append(stack_sa(state, action))\n",
    "            Samples['R'].append(reward)\n",
    "            Samples['Qnext'].append(self.Qnet.use(stack_sa(state, action)))\n",
    "            if bust(self.player_hand):\n",
    "                break\n",
    "\n",
    "        dealers_play(self.dealer_hand, self.other_hands, self.shoe)\n",
    "\n",
    "        reward = self.reinforcement_f(self.player_hand, self.dealer_hand, True)\n",
    "        Samples['R'][-1] = reward\n",
    "        Samples['Qnext'][-1] = self.Qnet.use(stack_sa(state, -1))\n",
    "        \n",
    "        Samples['SA'] = np.vstack(Samples['SA'])\n",
    "        Samples['R'] = np.array(Samples['R']).reshape(-1, 1)\n",
    "        Samples['Qnext'] = np.array(Samples['Qnext']).reshape(-1, 1)\n",
    "        return Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Casino Blackjack Qnets\n",
    "\n",
    "Much like before, we will train a few Qnets to see how they perform.\n",
    "\n",
    "Inerpreting the output is similar to before, but now we have a different desired reinforcement value, since the function is fundamentally different.\n",
    "0 means that the agent wins as often as it loses. The range of outcomes is in the interval [-1, 1] with -1 being always losing and 1 being always winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 5], 200, 0.01, -0.48]\n",
      "[[5, 5], 200, 0.001, -0.5935]\n",
      "[[5, 5], 100, 0.01, -0.512875]\n",
      "[[5, 5], 100, 0.001, -0.403125]\n",
      "[[10], 200, 0.01, -0.366375]\n",
      "[[10], 200, 0.001, -0.514125]\n",
      "[[10], 100, 0.01, -0.549625]\n",
      "[[10], 100, 0.001, -0.76925]\n",
      "[[10, 5, 10], 200, 0.01, -0.50475]\n",
      "[[10, 5, 10], 200, 0.001, -0.51]\n",
      "[[10, 5, 10], 100, 0.01, -0.574]\n",
      "[[10, 5, 10], 100, 0.001, -0.564375]\n"
     ]
    }
   ],
   "source": [
    "runner = group_play_runner(Shoe(1), reinforcement_group, 0.1)\n",
    "runner.run([[5,5],[10],[10,5,10]], [200,100], [.01,.001], 1, 8000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these experiments with this reinforment function, we can interpret a result of 0 as an equal number of wins and losses.\n",
    "These results show that the models are not able to consistantly beat the dealer as often as they lose.\n",
    "While the dealer has a slight advantage in the game, this is typically around a few percent, leading to the belief that the Qnets are not learning the game very well.\n",
    "\n",
    "There a few considerations as to why this might be happening.\n",
    "First the input space is rather large and some of the information is tied together, like the seperate hands.\n",
    "Other information has a very different meaning.\n",
    "The Qnet might have an easier time if we could organize the data in a more meaningful way.\n",
    "\n",
    "### Autoencoding the input space\n",
    "\n",
    "Aces play a special role in the game, so should be treated differently.\n",
    "The next idea to imporove on this is to reduce the dimensionality of the input space by creating an autoencoder for the non-ace cards in a hand.\n",
    "The Qnet will then have its input space reduced by passing both the player hand and the other's hand to the autoencoder and stacking their results with the other parameters.\n",
    "\n",
    "Typical card counting methods boil down hand information into a single numerical value. We will allow the Qnet 3 parameters in hopes that it might find a more meaningful way to represent the information.\n",
    "\n",
    "This will use a given file `neuralnetworks_torch.py` to speed up training and to utilize the `use_to_middle` method for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import neuralnetworks_torch as nntorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build an autoencoder for the non-ace cards.\n",
    "We will need it to be in $N^{10}$.\n",
    "We must create a random generator for the autoencoder to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_10_vector():\n",
    "    spread = np.random.randint(0, 12, 10)\n",
    "    return np.abs(spread)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the encoder we will generate batches of hands using our `random_10_vector` function.\n",
    "The below function will specify the number of such per batch and the number of batches to iterate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto(n_hidden, n_epoch, learning_rate, batch_size, num_batches):\n",
    "    net = nntorch.NeuralNetwork(10, n_hidden, 10, ['relu']*len(n_hidden), device='cuda')\n",
    "    for b in range(num_batches):\n",
    "        X = torch.from_numpy(np.vstack([random_10_vector() for _ in range(batch_size)])).float().to(net.device)\n",
    "        T = X\n",
    "        if b % 100 == 0:\n",
    "            print('batch: ', b)\n",
    "            error = net.use(X) - T.detach().cpu().numpy()\n",
    "            print('RMSE: ', np.sqrt(np.mean(error**2)))\n",
    "        net.fit(X, T, n_epoch, method='adam', learning_rate=learning_rate, verbose=False)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "RMSE:  6.6893597\n",
      "batch:  100\n",
      "RMSE:  2.8917954\n",
      "batch:  200\n",
      "RMSE:  2.868177\n",
      "batch:  300\n",
      "RMSE:  2.9151592\n",
      "batch:  400\n",
      "RMSE:  2.8944354\n",
      "batch:  500\n",
      "RMSE:  2.870629\n",
      "batch:  600\n",
      "RMSE:  2.9048617\n",
      "batch:  700\n",
      "RMSE:  2.881488\n",
      "batch:  800\n",
      "RMSE:  2.9062338\n",
      "batch:  900\n",
      "RMSE:  2.8914313\n",
      "batch:  1000\n",
      "RMSE:  2.9289382\n",
      "batch:  1100\n",
      "RMSE:  2.9032214\n",
      "batch:  1200\n",
      "RMSE:  2.876069\n",
      "batch:  1300\n",
      "RMSE:  2.8815217\n",
      "batch:  1400\n",
      "RMSE:  2.847389\n",
      "batch:  1500\n",
      "RMSE:  2.854869\n",
      "batch:  1600\n",
      "RMSE:  2.8949654\n",
      "batch:  1700\n",
      "RMSE:  2.8776548\n",
      "batch:  1800\n",
      "RMSE:  2.928284\n",
      "batch:  1900\n",
      "RMSE:  2.8965008\n"
     ]
    }
   ],
   "source": [
    "auto = train_auto([3], 150, .01, 1000, 2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above many other architectures were tried and most of them performed at most this well.\n",
    "Below the version of blackjack that utilized this was also tried and no significant improvements were seen.\n",
    "I am remiss to have left this information out of this report.\n",
    "Instead I will simply state that the results were not significant enough to warrant further investigation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is notable that the number of combinations of just 0 and 1 in a vector of ten is very large.\n",
    "So to try to improve this I will attempt to do a few layers of autoencoding.\n",
    "2 three dimensional encodings and a a 4 dimensional encoding, each with two outputs will give us 6 outputs.\n",
    "We can then use the three dimensional autoencoders to reduce this to 4 outs.\n",
    "\n",
    "There is little reason to believe this will work better than the previous technique in the final product.\n",
    "Each encoder should be more robust than the above one, but whether the dilution of information will be worth it is yet to be seen.\n",
    "For the number batch sizes a good number can be calculated since the number of possible configurations of vectors are small enough to be tenable.\n",
    "If we allow for 4 shoes, that means each card can appear 16 times.\n",
    "So for the 3 and 4 dimensional autoencoders repectively we will have $16^3$ and $16^4$ possible configurations.\n",
    "To allow for this we will do batches of 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_n_vector(n):\n",
    "    spread = np.random.randint(0, 12, n)\n",
    "    return np.abs(spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_auto(n_hidden, n_epoch, learning_rate, batch_size, num_batches, n_inputs):\n",
    "    net = nntorch.NeuralNetwork(n_inputs, n_hidden, n_inputs, ['relu']*len(n_hidden), device='cuda')\n",
    "    for b in range(num_batches):\n",
    "        X = torch.from_numpy(np.vstack([random_n_vector(n_inputs) for _ in range(batch_size)])).float().to(net.device)\n",
    "        T = X\n",
    "        if b % 100 == 0:\n",
    "            print('batch: ', b)\n",
    "            error = net.use(X) - T.detach().cpu().numpy()\n",
    "            print('RMSE: ', np.sqrt(np.mean(error**2)))\n",
    "        net.fit(X, T, n_epoch, method='adam', learning_rate=learning_rate, verbose=False)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "RMSE:  6.4573255\n",
      "batch:  100\n",
      "RMSE:  1.9946792\n",
      "batch:  200\n",
      "RMSE:  1.994986\n",
      "batch:  300\n",
      "RMSE:  1.9992826\n",
      "batch:  400\n",
      "RMSE:  1.9920868\n",
      "batch:  500\n",
      "RMSE:  1.9896367\n",
      "batch:  600\n",
      "RMSE:  1.9925561\n",
      "batch:  700\n",
      "RMSE:  1.9958317\n",
      "batch:  800\n",
      "RMSE:  1.9936018\n",
      "batch:  900\n",
      "RMSE:  1.993729\n"
     ]
    }
   ],
   "source": [
    "#Note this is incorrectly named, but to avoid retraining the network, I'm leaving it as is\n",
    "auto_3_to_1 = train_n_auto([2], 200, .01, 100000, 1000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "RMSE:  6.401102\n",
      "batch:  100\n",
      "RMSE:  1.9265873\n",
      "batch:  200\n",
      "RMSE:  1.9319617\n",
      "batch:  300\n",
      "RMSE:  1.9015895\n",
      "batch:  400\n",
      "RMSE:  1.8568105\n",
      "batch:  500\n",
      "RMSE:  1.5665952\n",
      "batch:  600\n",
      "RMSE:  1.5576278\n",
      "batch:  700\n",
      "RMSE:  1.561325\n",
      "batch:  800\n",
      "RMSE:  1.559476\n",
      "batch:  900\n",
      "RMSE:  1.5583557\n"
     ]
    }
   ],
   "source": [
    "auto_4_2 = train_n_auto([4,3,2,3,4], 200, .01, 100000, 1000, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These smaller autoencoders are performing better than the large one.\n",
    "I recognize here that the error in encoding may be incurred on each layer of encoding, so the reduction in error in these smaller ones may not be as significant as it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_vector_reduce_by_encoding(hand):\n",
    "    ace = hand[0]\n",
    "    face_through_8 = np.hstack([hand[1], hand[-1:-4]])\n",
    "    two_through_4 = hand[4:7]\n",
    "    five_through_7 = hand[7:10]\n",
    "\n",
    "    encoding_layer0 = auto_4_2.use_to_middle(torch.from_numpy(face_through_8).float().to(auto_4_2.device))\n",
    "    encoding_layer1 = auto_3_to_1.use_to_middle(torch.from_numpy(two_through_4).float().to(auto_3_to_1.device))\n",
    "    encoding_layer2 = auto_3_to_1.use_to_middle(torch.from_numpy(five_through_7).float().to(auto_3_to_1.device))\n",
    "\n",
    "    full_encoding_layer = np.hstack([encoding_layer0, encoding_layer1, encoding_layer2])\n",
    "    \n",
    "    encoding_outs1 = auto_3_to_1.use_to_middle(full_encoding_layer[:3])\n",
    "    encoding_outs2 = auto_3_to_1.use_to_middle(full_encoding_layer[3:])\n",
    "\n",
    "    return np.hstack([ace, encoding_outs1, encoding_outs2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class group_play_auto_runner(group_play_runner):\n",
    "    def __init__(self, shoe, reinforcement_f, gamma):\n",
    "        super().__init__(shoe, reinforcement_f, gamma)\n",
    "\n",
    "    def get_input_vector(self, args):\n",
    "        hand = args['hand']\n",
    "        dealer_hand = args['dealer_hand']\n",
    "        shoe = args['shoe']\n",
    "        others_hand = args['others']\n",
    "        player = make_hand_vector(hand)\n",
    "        player = hand_vector_reduce_by_encoding(player)\n",
    "        others =make_hand_vector(others_hand)\n",
    "        others = hand_vector_reduce_by_encoding(others)\n",
    "        dealer = make_dealer_hand_vector(dealer_hand)\n",
    "        \n",
    "        out =np.hstack([player, others, dealer, shoe.get_max_size() - len(shoe)])\n",
    "        return out\n",
    "\n",
    "    def get_signatures(self):\n",
    "        reduced_hand = 4\n",
    "        dealer_hand = 2\n",
    "        shoe_information = 1\n",
    "        ace = 1\n",
    "        num_actions = 1\n",
    "        return {\n",
    "            \"input_length\": reduced_hand*2 + dealer_hand + shoe_information + ace*2 + num_actions,\n",
    "            \"output_length\": 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10], 200, 0.01, -0.794125]\n",
      "[[5], 200, 0.01, -0.503]\n",
      "[[5, 10, 5], 200, 0.01, -0.6075]\n"
     ]
    }
   ],
   "source": [
    "runner = group_play_auto_runner(Shoe(1), reinforcement_group, 0.1)\n",
    "runner.run([[10,10],[5],[5,10,5]], [200], [.01], 1, 8000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is is not much better than the regular Qnet, if not slightly worse.\n",
    "The Qnets with the autoencoders took significantly longer to train as well.\n",
    "We may expect a better result with this type of approach if the autoencoders were able to be more precisely tuned to reduce the error.\n",
    "\n",
    "A potentially more fruitful approach would be to use convolutional layers of the portions that are meant to be grouped together.\n",
    "Here the approach would simply be to take the `player hand` and `other players hand` and treat them both independently for the first few layers.\n",
    "Then we could then combine these outputs with the remaining parameters and feed this input space into a fully connected portion.\n",
    "In the interest of time, this venture will be aschewed for the scope of this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a few hands with the best Qnet\n",
    "\n",
    "To interpret the reinforcement values, we treat it the same as before, such that numbers closer to 1 are preferable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 5], 100, 0.001, -0.392875]\n"
     ]
    }
   ],
   "source": [
    "runner = group_play_runner(Shoe(1), reinforcement_group, 0.1)\n",
    "runner.run([[5,5]], [100], [.001], 1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qnet = runner.get_Qnet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one we will have a similar looking output.\n",
    "We won't announce the other players hands, since they ddon't matter to us.\n",
    "We will however announce the players hands and the dealers hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoe_group = Shoe(2)\n",
    "other_hands = []\n",
    "\n",
    "\n",
    "def play_group(Qnet, shoe, get_input_vector):\n",
    "        player_hand = []\n",
    "        dealer_hand = []\n",
    "\n",
    "        n_others = get_num_of_other_players(shoe)\n",
    "\n",
    "        deal(shoe, player_hand, other_hands, dealer_hand, n_others)\n",
    "        print('After dealing the player has the hand: ', player_hand)\n",
    "\n",
    "        for _ in range(n_others):\n",
    "            other_players_play(other_hands, shoe)\n",
    "\n",
    "        while True:\n",
    "            state = get_input_vector({\n",
    "                \"hand\": player_hand,\n",
    "                \"others\": other_hands,\n",
    "                \"dealer_hand\": dealer_hand,\n",
    "                \"shoe\": shoe,\n",
    "            })\n",
    "            action = epsilon_greedy(Qnet, state, 0, player_hand)\n",
    "            \n",
    "            if action == 1:\n",
    "                print('stand')\n",
    "                print('Final score: ', hand_value(player_hand))\n",
    "                break\n",
    "        \n",
    "            hit(player_hand, shoe)\n",
    "\n",
    "            print('hit', player_hand)\n",
    "\n",
    "            if bust(player_hand):\n",
    "                print('bust')\n",
    "                break\n",
    "\n",
    "        dealers_play(dealer_hand, other_hands, shoe)\n",
    "        print('Dealer hand: ', dealer_hand)\n",
    "        print('Dealer score: ', hand_value(dealer_hand))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dealing the player has the hand:  ['King', '2']\n",
      "hit ['King', '2', '8']\n",
      "hit ['King', '2', '8', 'Queen']\n",
      "bust\n",
      "Dealer hand:  ['Jack', 'King']\n",
      "Dealer score:  20\n",
      "------------------\n",
      "After dealing the player has the hand:  ['King', '3']\n",
      "stand\n",
      "Final score:  13\n",
      "Dealer hand:  ['5', '2', '7', '5']\n",
      "Dealer score:  19\n",
      "------------------\n",
      "After dealing the player has the hand:  ['10', 'Jack']\n",
      "stand\n",
      "Final score:  20\n",
      "Dealer hand:  ['Ace', '7']\n",
      "Dealer score:  18\n",
      "------------------\n",
      "After dealing the player has the hand:  ['10', '4']\n",
      "stand\n",
      "Final score:  14\n",
      "Dealer hand:  ['Queen', '3', 'Ace', '3']\n",
      "Dealer score:  17\n",
      "------------------\n",
      "After dealing the player has the hand:  ['9', '10']\n",
      "stand\n",
      "Final score:  19\n",
      "Dealer hand:  ['2', 'Queen', '10']\n",
      "Dealer score:  22\n",
      "------------------\n",
      "After dealing the player has the hand:  ['9', 'King']\n",
      "stand\n",
      "Final score:  19\n",
      "Dealer hand:  ['5', '9', '10']\n",
      "Dealer score:  24\n",
      "------------------\n",
      "After dealing the player has the hand:  ['King', '10']\n",
      "stand\n",
      "Final score:  20\n",
      "Dealer hand:  ['4', '3', '2', 'Queen']\n",
      "Dealer score:  19\n",
      "------------------\n",
      "After dealing the player has the hand:  ['6', '9']\n",
      "stand\n",
      "Final score:  15\n",
      "Dealer hand:  ['5', 'Jack', 'Ace', '7']\n",
      "Dealer score:  23\n",
      "------------------\n",
      "After dealing the player has the hand:  ['4', '5']\n",
      "stand\n",
      "Final score:  9\n",
      "Dealer hand:  ['Ace', '3', '8', '6']\n",
      "Dealer score:  18\n",
      "------------------\n",
      "After dealing the player has the hand:  ['4', 'Queen']\n",
      "stand\n",
      "Final score:  14\n",
      "Dealer hand:  ['Jack', '2', '7']\n",
      "Dealer score:  19\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    play_group(Qnet, shoe_group, runner.get_input_vector)\n",
    "    print('------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the behavior of the model does not vary much from our solo blackjack model.\n",
    "\n",
    "### Quick aside:\n",
    "\n",
    "At this point it occurs to me that the samples were generating only a single hand at a time, which likely led to suboptimal learning.\n",
    "In response to this I adpated the solo runner to generate a batch of hands at a time.\n",
    "After running some preliminary experiments with both the linear and nonlinear reward functions, it appears the the results were not significantly different.\n",
    "I will for that reason leave the above section as is.\n",
    "\n",
    "In the case of the group runner, below we will see a marked improvement in the results.\n",
    "This is accomplished with the following slight alteration to the runner to allow for bigger sample batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class group_play_runner_exta_samples(group_play_runner):\n",
    "    def get_samples(self):\n",
    "        sample_hands = []\n",
    "        for _ in range(20):\n",
    "            sample_hands.append(super().get_samples())\n",
    "        \n",
    "        out = {\n",
    "            \"SA\": [],\n",
    "            \"R\": [],\n",
    "            \"Qnext\": []\n",
    "        }\n",
    "        for key in out.keys():\n",
    "            out[key] = np.vstack([sample[key] for sample in sample_hands])\n",
    "        return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10], 100, 0.01, -0.529125]\n",
      "[[10, 10], 50, 0.01, -0.319375]\n",
      "[[5], 100, 0.01, -0.393625]\n",
      "[[5], 50, 0.01, -0.228]\n",
      "[[5, 10, 5], 100, 0.01, -0.445625]\n",
      "[[5, 10, 5], 50, 0.01, -0.574125]\n"
     ]
    }
   ],
   "source": [
    "runner1 = group_play_runner_exta_samples(Shoe(1), reinforcement_group, 0.1)\n",
    "runner1.run([[10,10],[5],[5,10,5]], [100, 50], [.01], 1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], 50, 0.01, -0.42475]\n"
     ]
    }
   ],
   "source": [
    "runner = group_play_runner_exta_samples(Shoe(1), reinforcement_group, 0.1)\n",
    "runner.run([[5]], [50], [.01], 1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dealing the player has the hand:  ['5', '8']\n",
      "stand\n",
      "Final score:  13\n",
      "Dealer hand:  ['Queen', '7']\n",
      "Dealer score:  17\n",
      "------------------\n",
      "After dealing the player has the hand:  ['10', 'Jack']\n",
      "stand\n",
      "Final score:  20\n",
      "Dealer hand:  ['Jack', '9']\n",
      "Dealer score:  19\n",
      "------------------\n",
      "After dealing the player has the hand:  ['7', 'Queen']\n",
      "stand\n",
      "Final score:  17\n",
      "Dealer hand:  ['6', 'Queen', '8']\n",
      "Dealer score:  24\n",
      "------------------\n",
      "After dealing the player has the hand:  ['Ace', '5']\n",
      "hit ['Ace', '5', '6']\n",
      "hit ['Ace', '5', '6', 'King']\n",
      "bust\n",
      "Dealer hand:  ['Ace', 'Jack']\n",
      "Dealer score:  21\n",
      "------------------\n",
      "After dealing the player has the hand:  ['King', 'Queen']\n",
      "hit ['King', 'Queen', '4']\n",
      "bust\n",
      "Dealer hand:  ['Jack', '9']\n",
      "Dealer score:  19\n",
      "------------------\n",
      "After dealing the player has the hand:  ['8', '2']\n",
      "hit ['8', '2', '7']\n",
      "hit ['8', '2', '7', '10']\n",
      "bust\n",
      "Dealer hand:  ['8', '7', '10']\n",
      "Dealer score:  25\n",
      "------------------\n",
      "After dealing the player has the hand:  ['4', 'Jack']\n",
      "hit ['4', 'Jack', 'King']\n",
      "bust\n",
      "Dealer hand:  ['7', '2', '5', 'Ace', 'Jack']\n",
      "Dealer score:  25\n",
      "------------------\n",
      "After dealing the player has the hand:  ['2', '10']\n",
      "hit ['2', '10', 'Jack']\n",
      "bust\n",
      "Dealer hand:  ['3', '9', 'Ace', '3', 'Ace']\n",
      "Dealer score:  17\n",
      "------------------\n",
      "After dealing the player has the hand:  ['8', '9']\n",
      "hit ['8', '9', '9']\n",
      "bust\n",
      "Dealer hand:  ['King', 'Queen']\n",
      "Dealer score:  20\n",
      "------------------\n",
      "After dealing the player has the hand:  ['6', '3']\n",
      "hit ['6', '3', '9']\n",
      "hit ['6', '3', '9', '10']\n",
      "bust\n",
      "Dealer hand:  ['4', '10', '7']\n",
      "Dealer score:  21\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "Qnet = runner.get_Qnet()\n",
    "shoe_group = Shoe(2)\n",
    "other_hands = []\n",
    "\n",
    "for _ in range(10):\n",
    "    play_group(Qnet, shoe_group, runner.get_input_vector)\n",
    "    print('------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this alteration to the autoencoder runner as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class group_play_auto_extra_runner(group_play_auto_runner):\n",
    "    def get_samples(self):\n",
    "        sample_hands = []\n",
    "        for _ in range(20):\n",
    "            sample_hands.append(super().get_samples())\n",
    "        \n",
    "        out = {\n",
    "            \"SA\": [],\n",
    "            \"R\": [],\n",
    "            \"Qnext\": []\n",
    "        }\n",
    "        for key in out.keys():\n",
    "            out[key] = np.vstack([sample[key] for sample in sample_hands])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], 50, 0.01, -0.379375]\n",
      "[[5, 5], 50, 0.01, -0.436625]\n"
     ]
    }
   ],
   "source": [
    "runner = group_play_auto_extra_runner(Shoe(1), reinforcement_group, 0.1)\n",
    "runner.run([[5],[5,5]], [50], [.01], 1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = runner.get_Qnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dealing the player has the hand:  ['9', 'Queen']\n",
      "stand\n",
      "Final score:  19\n",
      "Dealer hand:  ['6', 'Jack', 'King']\n",
      "Dealer score:  26\n",
      "------------------\n",
      "After dealing the player has the hand:  ['9', 'Queen']\n",
      "stand\n",
      "Final score:  19\n",
      "Dealer hand:  ['Ace', '10']\n",
      "Dealer score:  21\n",
      "------------------\n",
      "After dealing the player has the hand:  ['5', '2']\n",
      "stand\n",
      "Final score:  7\n",
      "Dealer hand:  ['8', '6', '10']\n",
      "Dealer score:  24\n",
      "------------------\n",
      "After dealing the player has the hand:  ['5', '2']\n",
      "stand\n",
      "Final score:  7\n",
      "Dealer hand:  ['3', 'Ace', '8', '3', 'Ace', 'Ace']\n",
      "Dealer score:  17\n",
      "------------------\n",
      "After dealing the player has the hand:  ['Queen', '6']\n",
      "stand\n",
      "Final score:  16\n",
      "Dealer hand:  ['Jack', 'Jack']\n",
      "Dealer score:  20\n",
      "------------------\n",
      "After dealing the player has the hand:  ['3', 'Jack']\n",
      "stand\n",
      "Final score:  13\n",
      "Dealer hand:  ['5', '4', '4', '4']\n",
      "Dealer score:  17\n",
      "------------------\n",
      "After dealing the player has the hand:  ['King', '7']\n",
      "stand\n",
      "Final score:  17\n",
      "Dealer hand:  ['7', '8', '5']\n",
      "Dealer score:  20\n",
      "------------------\n",
      "After dealing the player has the hand:  ['4', '3']\n",
      "stand\n",
      "Final score:  7\n",
      "Dealer hand:  ['10', 'Queen']\n",
      "Dealer score:  20\n",
      "------------------\n",
      "After dealing the player has the hand:  ['2', 'King']\n",
      "stand\n",
      "Final score:  12\n",
      "Dealer hand:  ['3', '10', '3', '6']\n",
      "Dealer score:  22\n",
      "------------------\n",
      "After dealing the player has the hand:  ['6', '8']\n",
      "stand\n",
      "Final score:  14\n",
      "Dealer hand:  ['5', '5', '4', '9']\n",
      "Dealer score:  23\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "shoe_group = Shoe(2)\n",
    "other_hands = []\n",
    "\n",
    "for _ in range(10):\n",
    "    play_group(qnet, shoe_group, runner.get_input_vector)\n",
    "    print('------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we see that the autoencoded version returns to the hyper-conservative strategy which has been very hard to keep from emerging.\n",
    "Still the results are superior to the versions which were trained with a single hand at a time.\n",
    "While only a few hands are analysed here, the expectation given this result is that in certain conditions, the model has learned decent startegies, while in many others it does not.\n",
    "Perhaps this better performance ratio is a result of averaging among such circumstances where its amount of knowledge varies/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the project has been given substantial consideration.\n",
    "In order to achieve the betting behavior, two approaches were considered.\n",
    "The first was to train a seperate network to watch the agent and place bets based on its behavior.\n",
    "The second was to incorporate an extra output into the existing network.\n",
    "\n",
    "While the 2 network approach would reduce the internal complexity of the function approximation, it would substantially increase training time.\n",
    "The single network approach may have an advantage of giving information about the risk associated with a given hand.\n",
    "While, the running bet can be added to the player agent while maintaining a seperate network for betting, having all the information embedded in the weights of a single network may give an advantage in processing the meaning of this parameter.\n",
    "\n",
    "With these considerations in mind, the single network approach was chosen.\n",
    "We will add an extra parameter to the input space which will be the running bet.\n",
    "On the initial deal, this value will be zero.\n",
    "This is sensical because in reality the agent would not be able to play without an anti of some kind.\n",
    "In the first query, the agent will be probed for a bet as a second output.\n",
    "After this, the bet will remain as an input for all following queries in the round, and the possible actions will be adjusted to reflect the impossibility of changing that bet.\n",
    "\n",
    "Again we will need to adapt our reward function for this new environment.\n",
    "Two seperate approaches will be compared.\n",
    "1. The reward will be the final amount of money the agent has after several hands and the agent will be trained to get the most winnings over several games.\n",
    "2. The reward will be taken at a per hand basis and simply be the amount of money won or lost in that hand.\n",
    "At this point, I do not have any reasonable way to predict which of these will be better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have to adapt our epsilon greedy function to handle the increased input space.\n",
    "We note that both action parameters are essentially orthogonal because the net will never be asked to bet and move at the same time.\n",
    "Therefore we find the valid actions will only be checked at one axis at a time epending on if it is the first move.\n",
    "The chosen action will be placed into the vector of size 2 using the `get_effective_action` function.\n",
    "This allows the epsilon greedy function to be agnostic to the action being queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hand parameter here will be expanded into a dictionary so we can account for the purse and the consideration of whether it is time to bet\n",
    "def valid_betting_actions(hand):\n",
    "    purse = hand['purse']\n",
    "    first_move = hand['first_move']\n",
    "    player_hand = hand['player_hand']\n",
    "    move = valid_actions(player_hand)\n",
    "    \n",
    "    if first_move:\n",
    "        return [bet for bet in range(20, int(purse + 1), 20)]\n",
    "    else:\n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_action(action_of_interest, location_of_action):\n",
    "    action = np.zeros(2)\n",
    "    action[location_of_action] = action_of_interest\n",
    "    return action\n",
    "\n",
    "# epsilon greedy needs to be adapted slightly to have multiple actions\n",
    "def epsilon_greedy_multi(Qnet, state, epsilon, hand):\n",
    "\n",
    "    first_move = hand['first_move']\n",
    "    location_of_action = 0\n",
    "    if first_move:\n",
    "        location_of_action = 1\n",
    "\n",
    "    actions = valid_betting_actions(hand)\n",
    "    \n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        action_of_interest = np.random.choice(actions)\n",
    "        action = get_effective_action(action_of_interest, location_of_action)\n",
    "        \n",
    "    else:\n",
    "        # Greedy Move\n",
    "        np.random.shuffle(actions)\n",
    "        effective_actions = [get_effective_action(action_of_interest, location_of_action) for action_of_interest in actions]\n",
    "        Qs = np.array([Qnet.use(stack_sa(state, a)) for a in effective_actions])\n",
    "        action = actions[np.argmax(Qs)]\n",
    "        action = get_effective_action(action, location_of_action)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomodate the two reinforcement approaches, we will use the following `get_bet_reinforcement` closure.\n",
    "It can be given a string to specify if we are using the \"short term\" or \"long term\" reinforcement.\n",
    "we can then share the related code and chose the actual return by fetching the appropriate function from the closure.\n",
    "\n",
    "Note: the reinforcement functions yielded here make changes to the `new_purse` parameter of the given `runner` object.\n",
    "This is useful because the calculation of this value is needed for gameplay flow control.\n",
    "The runner may then reset its `purse` to the `new_purse` value when desired provided the reinforcement was just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_loss(player_hand, dealer_hand):\n",
    "    if bust(player_hand):\n",
    "        return False\n",
    "    elif bust(dealer_hand):\n",
    "        return True\n",
    "    else:\n",
    "        return hand_value(player_hand) > hand_value(dealer_hand)\n",
    "    \n",
    "\n",
    "def get_bet_reinforcement(strategy):\n",
    "    def bet_reinforcement(runner):\n",
    "        bet = runner.bet\n",
    "        net_gain = 0\n",
    "        if win_loss(runner.player_hand, runner.dealer_hand):\n",
    "            net_gain = bet\n",
    "        else:\n",
    "            net_gain = -bet\n",
    "\n",
    "        runner.new_purse = runner.purse + net_gain\n",
    "\n",
    "    \n",
    "        if strategy == 'short term':\n",
    "            return net_gain\n",
    "        elif strategy == 'long term':\n",
    "            return runner.purse\n",
    "\n",
    "    return bet_reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforment_short = get_bet_reinforcement('short term')\n",
    "reinforment_long = get_bet_reinforcement('long term')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extra consideration in the runner, we learned that having several hands at a time was beneficial to the learning process.\n",
    "A problem with this is that the final evaluation of the net can be skewed if it fails and gets a full purse back by the end.\n",
    "To fix this we will attempt 20 hands per training session but stop the sample set generation anytime the agent goes broke.\n",
    "Not only does this fix our evaluation metric but it doesn't delude the network into thinking it can get free money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bet_runner(group_play_runner):\n",
    "    def __init__(self, shoe, reinforcement_f, gamma):\n",
    "        super().__init__(shoe, reinforcement_f, gamma)\n",
    "        self.bet = 0\n",
    "        self.purse = 1000\n",
    "        self.new_purse = 1000\n",
    "\n",
    "    def get_input_vector(self, args):\n",
    "        hand = args['hand']\n",
    "        dealer_hand = args['dealer_hand']\n",
    "        shoe = args['shoe']\n",
    "        others_hand = args['others']\n",
    "        bet = args['bet']\n",
    "        current_purse = args['purse']\n",
    "\n",
    "        player = make_hand_vector(hand)\n",
    "        others =make_hand_vector(others_hand)\n",
    "        dealer = make_dealer_hand_vector(dealer_hand)\n",
    "        \n",
    "        out =np.hstack([player, others, dealer, shoe.get_max_size() - len(shoe), bet, current_purse])\n",
    "        return out\n",
    "\n",
    "    def get_signatures(self):\n",
    "        hand = 11\n",
    "        dealer_hand = 2\n",
    "        shoe_information = 1\n",
    "        current_bet = 1\n",
    "        purse = 1\n",
    "        num_actions = 2\n",
    "\n",
    "        return {\n",
    "            \"input_length\": hand*2 + dealer_hand + shoe_information + current_bet + purse + num_actions,\n",
    "            \"output_length\": 1\n",
    "        }\n",
    "    \n",
    "    def get_bet(self):\n",
    "        # We want the dealer and player hands to be zeros here because we don't know them yet. Still we know the running count so we can use the other's hand data. We can make the zeros of the right size by passing empty lists.\n",
    "        input = self.get_input_vector(\n",
    "            {\n",
    "                \"hand\": [],\n",
    "                \"others\": self.other_hands,\n",
    "                \"dealer_hand\": [],\n",
    "                \"shoe\": self.shoe,\n",
    "                \"bet\": self.bet,\n",
    "                \"purse\": self.purse\n",
    "            }\n",
    "        )\n",
    "\n",
    "        detailed_hand = {\n",
    "            \"player_hand\": self.player_hand,\n",
    "            \"purse\": self.purse,\n",
    "            \"first_move\": True,\n",
    "        }\n",
    "\n",
    "        bet = epsilon_greedy_multi(self.Qnet, input, self.epsilon, detailed_hand)\n",
    "        return bet[1]\n",
    "\n",
    "\n",
    "    def get_samples_hand(self):\n",
    "        Samples = { \"SA\": [], \"R\": [], \"Qnext\": [] }\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "\n",
    "        if self.purse <= 0:\n",
    "            self.purse = 1000\n",
    "            self.shoe = Shoe(np.random.randint(1, 7))\n",
    "            self.other_hands = []\n",
    "            return {\n",
    "                'samples': Samples,\n",
    "                'broke': True\n",
    "            }\n",
    "\n",
    "        n_others = get_num_of_other_players(self.shoe)\n",
    "        if is_shoe_done(self.shoe):\n",
    "            #number of decks between 1 and 6\n",
    "            self.shoe = Shoe(np.random.randint(1, 7))\n",
    "            self.other_hands = []\n",
    "\n",
    "        self.bet = self.get_bet() \n",
    "\n",
    "        deal(self.shoe, self.player_hand, self.other_hands, self.dealer_hand, n_others)\n",
    "\n",
    "        for _ in range(n_others):\n",
    "            other_players_play(self.other_hands, self.shoe)\n",
    "\n",
    "        while True:\n",
    "            state = self.get_input_vector({\n",
    "                \"hand\": self.player_hand,\n",
    "                \"others\": self.other_hands,\n",
    "                \"dealer_hand\": self.dealer_hand,\n",
    "                \"shoe\": self.shoe,\n",
    "                \"bet\": self.bet,\n",
    "                \"purse\": self.purse\n",
    "            })\n",
    "\n",
    "            detailed_hand = {\n",
    "                \"player_hand\": self.player_hand,\n",
    "                \"purse\": self.purse,\n",
    "                \"first_move\": False\n",
    "            }\n",
    "\n",
    "            action = epsilon_greedy_multi(self.Qnet, state, self.epsilon, detailed_hand)\n",
    "            \n",
    "            if action[0] == 1:\n",
    "                reward = self.reinforcement_f(self)\n",
    "                Samples['SA'].append(stack_sa(state, action))\n",
    "                Samples['R'].append(reward)\n",
    "                Samples['Qnext'].append(self.Qnet.use(stack_sa(state, action)))\n",
    "                break\n",
    "        \n",
    "            hit(self.player_hand, self.shoe)\n",
    "\n",
    "            reward = self.reinforcement_f(self)\n",
    "            Samples['SA'].append(stack_sa(state, action))\n",
    "            Samples['R'].append(reward)\n",
    "            Samples['Qnext'].append(self.Qnet.use(stack_sa(state, action)))\n",
    "            if bust(self.player_hand):\n",
    "                break\n",
    "\n",
    "        dealers_play(self.dealer_hand, self.other_hands, self.shoe)\n",
    "\n",
    "        reward = self.reinforcement_f(self)\n",
    "        self.purse = self.new_purse\n",
    "        Samples['R'][-1] = reward\n",
    "        Samples['Qnext'][-1] = self.Qnet.use(stack_sa(state, [-1, -1]))\n",
    "        \n",
    "        Samples['SA'] = np.vstack(Samples['SA'])\n",
    "        Samples['R'] = np.array(Samples['R']).reshape(-1, 1)\n",
    "        Samples['Qnext'] = np.array(Samples['Qnext']).reshape(-1, 1)\n",
    "        return {\n",
    "            'samples': Samples,\n",
    "            'broke': False\n",
    "        }\n",
    "    \n",
    "    def get_samples(self):\n",
    "        sample_hands = []\n",
    "        for _ in range(20):\n",
    "            sample_hand = self.get_samples_hand()\n",
    "            if sample_hand['broke']:\n",
    "                break\n",
    "            sample_hands.append(sample_hand['samples'])\n",
    "        \n",
    "        out = {\n",
    "            \"SA\": [],\n",
    "            \"R\": [],\n",
    "            \"Qnext\": []\n",
    "        }\n",
    "        for key in out.keys():\n",
    "            out[key] = np.vstack([sample[key] for sample in sample_hands])\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the betting Qnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One Hand Reinforcement*\n",
    "\n",
    "For starters, lets focus on the one hand at a time reinforment function.\n",
    "The number of rounds is decreased here from previous experiments because of the loop making several hands per round.\n",
    "This works out to be around the same amount of games in a training session.\n",
    "\n",
    "To evaluate the output, we interpret the reinforcement as a the amount of money made on average per hand.\n",
    "In reality we would want this to be positive, indicating that you are making money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], 50, 0.01, -152.88888888888889]\n",
      "[[5], 30, 0.01, -248.93333333333334]\n",
      "[[5, 5], 50, 0.01, -106.35555555555555]\n",
      "[[5, 5], 30, 0.01, -242.44444444444446]\n",
      "[[10, 5, 10], 50, 0.01, -294.8]\n",
      "[[10, 5, 10], 30, 0.01, -228.62222222222223]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_short, .6)\n",
    "runner.run([[5],[5,5], [10,5,10]], [50,30], [.01], 1, 450)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Qnet is definitely not performing in a way that someone would want to use in Vegas.\n",
    "On average it loses money every hand.\n",
    "I notice that the higher epoch hyperparameter is better, so I will do a few more experiments to try to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], 50, 0.01, -235.37777777777777]\n",
      "[[5], 80, 0.01, -272.0]\n",
      "[[5, 5], 50, 0.01, -260.0]\n",
      "[[5, 5], 80, 0.01, -217.15555555555557]\n",
      "[[10], 50, 0.01, -346.5777777777778]\n",
      "[[10], 80, 0.01, -198.22222222222223]\n",
      "[[10, 10], 50, 0.01, -445.06666666666666]\n",
      "[[10, 10], 80, 0.01, -161.55555555555554]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_short, .6)\n",
    "runner.run([[5],[5,5], [10], [10,10]], [50,80], [.01], 1, 450)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous best did not perform as well this time.\n",
    "Still, higher epochs seem to be better.\n",
    "I will try a few more experiments with higher epochs, with the [5,5] architecture, since it is the most consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 5], 80, 0.01, -264.1333333333333]\n",
      "[[5, 5], 100, 0.01, -234.48888888888888]\n",
      "[[5, 5], 150, 0.01, -191.6]\n",
      "[[5, 5], 200, 0.01, -208.35555555555555]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_short, .6)\n",
    "runner.run([[5,5]], [80, 100, 150, 200], [.01], 1, 450)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger epoch systems seem to work better.\n",
    "Since the [10,10] architecture worked better with more epochs too, we will do a quick experiment to consider this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10], 150, 0.01, -286.5777777777778]\n",
      "[[10, 10], 200, 0.01, -186.53333333333333]\n",
      "[[10, 10], 300, 0.01, -209.06666666666666]\n",
      "[[10, 10], 500, 0.01, -143.95555555555555]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_short, .6)\n",
    "runner.run([[10,10]], [150, 200, 300, 500], [.01], 1, 450)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we don't see much difference in these results versus the other architectures.\n",
    "For simplicity we will stick with our best performer.\n",
    "This is [10,10] with 500 epochs.\n",
    "We can use this as a basis for our next experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Multi Hand Reinforcement*\n",
    "\n",
    "Next we'll look at how the Qnets perform with reinforcement based on final purse size.\n",
    "We can intrepret the evaluation metric as the average amount of money the agent has at the end of the game.\n",
    "Starting with 1000, would mean anything above 1000 is a profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10], 500, 0.01, 752.8888888888889]\n",
      "[[10, 10], 200, 0.01, 696.9333333333333]\n",
      "[[20, 20], 500, 0.01, 644.9333333333333]\n",
      "[[20, 20], 200, 0.01, 707.2]\n",
      "[[10, 8, 10], 500, 0.01, 796.0]\n",
      "[[10, 8, 10], 200, 0.01, 551.4222222222222]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_long, .6)\n",
    "runner.run([[10,10],[20,20],[10,8,10]], [500,200], [.01], 1, 450)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a few hands with the betting Qnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before we can code up a function to play some hands with the betting Qnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_betting(Qnet,game_runner):\n",
    "        game_runner.player_hand = []\n",
    "        game_runner.dealer_hand = []\n",
    "\n",
    "        if game_runner.purse <= 0:\n",
    "            print('Player is broke')\n",
    "            game_runner.purse = 1000\n",
    "            game_runner.shoe = Shoe(np.random.randint(1, 7))\n",
    "            game_runner.other_hands = []\n",
    "\n",
    "        n_others = get_num_of_other_players(game_runner.shoe)\n",
    "        if is_shoe_done(game_runner.shoe):\n",
    "            #number of decks between 1 and 6\n",
    "            game_runner.shoe = Shoe(np.random.randint(1, 7))\n",
    "            game_runner.other_hands = []\n",
    "\n",
    "        game_runner.bet = game_runner.get_bet() \n",
    "        print('Player purse: ', game_runner.purse, ' this round')\n",
    "        print('Player bet: ', game_runner.bet, ' this round')\n",
    "\n",
    "        deal(game_runner.shoe, game_runner.player_hand, game_runner.other_hands, game_runner.dealer_hand, n_others)\n",
    "        print('Player initial hand: ', game_runner.player_hand)\n",
    "        print('Dealer initial hand: ', game_runner.dealer_hand)\n",
    "\n",
    "        for _ in range(n_others):\n",
    "            other_players_play(game_runner.other_hands, game_runner.shoe)\n",
    "\n",
    "        while True:\n",
    "            state = game_runner.get_input_vector({\n",
    "                \"hand\": game_runner.player_hand,\n",
    "                \"others\": game_runner.other_hands,\n",
    "                \"dealer_hand\": game_runner.dealer_hand,\n",
    "                \"shoe\": game_runner.shoe,\n",
    "                \"bet\": game_runner.bet,\n",
    "                \"purse\": game_runner.purse\n",
    "            })\n",
    "\n",
    "            detailed_hand = {\n",
    "                \"player_hand\": game_runner.player_hand,\n",
    "                \"purse\": game_runner.purse,\n",
    "                \"first_move\": False\n",
    "            }\n",
    "\n",
    "            action = epsilon_greedy_multi(game_runner.Qnet, state, 0, detailed_hand)\n",
    "            \n",
    "            if action[0] == 1:\n",
    "                print('Player stands')\n",
    "                break\n",
    "        \n",
    "            hit(game_runner.player_hand, game_runner.shoe)\n",
    "            print('Player hand: ', game_runner.player_hand)\n",
    "\n",
    "            if bust(game_runner.player_hand):\n",
    "                print('Player busts')\n",
    "                break\n",
    "\n",
    "        dealers_play(game_runner.dealer_hand, game_runner.other_hands, game_runner.shoe)\n",
    "        print('Final dealer hand: ', game_runner.dealer_hand)\n",
    "\n",
    "        game_runner.reinforcement_f(game_runner)\n",
    "        game_runner.purse = game_runner.new_purse\n",
    "        print('Player purse: ', game_runner.purse, ' after round')\n",
    "        print('--------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil train our choices of Qnet with more rounds to try and improve the result.\n",
    "\n",
    "*One Hand Reinforcement*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10], 500, 0.01, -610.0888888888888]\n"
     ]
    }
   ],
   "source": [
    "runner = bet_runner(Shoe(6), reinforment_short, .6)\n",
    "runner.run([[10,10]], [500], [.01], 1, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player purse:  1000  this round\n",
      "Player bet:  1000.0  this round\n",
      "Player initial hand:  ['7', '4']\n",
      "Dealer initial hand:  ['5', '5']\n",
      "Player stands\n",
      "Final dealer hand:  ['5', '5', 'Jack']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  600.0  this round\n",
      "Player initial hand:  ['Jack', 'Jack']\n",
      "Dealer initial hand:  ['10', '6']\n",
      "Player stands\n",
      "Final dealer hand:  ['10', '6', '8']\n",
      "1600.0\n",
      "Player purse:  1600.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  1600.0  this round\n",
      "Player bet:  1600.0  this round\n",
      "Player initial hand:  ['5', '10']\n",
      "Dealer initial hand:  ['3', 'Jack']\n",
      "Player hand:  ['5', '10', '9']\n",
      "Player busts\n",
      "Final dealer hand:  ['3', 'Jack', 'Queen']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  100.0  this round\n",
      "Player initial hand:  ['5', 'Ace']\n",
      "Dealer initial hand:  ['5', 'Ace']\n",
      "Player stands\n",
      "Final dealer hand:  ['5', 'Ace', 'King', 'Jack']\n",
      "1100.0\n",
      "Player purse:  1100.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  1100.0  this round\n",
      "Player bet:  1100.0  this round\n",
      "Player initial hand:  ['King', '10']\n",
      "Dealer initial hand:  ['10', '3']\n",
      "Player stands\n",
      "Final dealer hand:  ['10', '3', '8']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  920.0  this round\n",
      "Player initial hand:  ['7', '6']\n",
      "Dealer initial hand:  ['Queen', '3']\n",
      "Player hand:  ['7', '6', '6']\n",
      "Player hand:  ['7', '6', '6', '4']\n",
      "Player busts\n",
      "Final dealer hand:  ['Queen', '3', '6']\n",
      "80.0\n",
      "Player purse:  80.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  80.0  this round\n",
      "Player bet:  40.0  this round\n",
      "Player initial hand:  ['6', 'Jack']\n",
      "Dealer initial hand:  ['5', '10']\n",
      "Player stands\n",
      "Final dealer hand:  ['5', '10', 'King']\n",
      "120.0\n",
      "Player purse:  120.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  120.0  this round\n",
      "Player bet:  120.0  this round\n",
      "Player initial hand:  ['8', 'Jack']\n",
      "Dealer initial hand:  ['9', 'King']\n",
      "Player stands\n",
      "Final dealer hand:  ['9', 'King']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  1000.0  this round\n",
      "Player initial hand:  ['Queen', 'Jack']\n",
      "Dealer initial hand:  ['10', '2']\n",
      "Player stands\n",
      "Final dealer hand:  ['10', '2', '10']\n",
      "2000.0\n",
      "Player purse:  2000.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  2000.0  this round\n",
      "Player bet:  440.0  this round\n",
      "Player initial hand:  ['5', '9']\n",
      "Dealer initial hand:  ['3', '8']\n",
      "Player stands\n",
      "Final dealer hand:  ['3', '8', 'Jack']\n",
      "1560.0\n",
      "Player purse:  1560.0  after round\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "short_qnet = runner.Qnet\n",
    "for _ in range(10):\n",
    "    play_betting(short_qnet, runner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short term reinforcment turns out to be an overly aggressive better.\n",
    "The agent does not seem to see a problem with going all in or betting most of its money on any hand.\n",
    "As we can see clearly here, this is a horrible strategy.\n",
    "One hypothesis about this is that the short term reinforcement really motivates the agent to maximize its potential earnings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Long Term Reinforcement*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 8, 10], 200, 0.01, 716.4888888888889]\n"
     ]
    }
   ],
   "source": [
    "runner_l = bet_runner(Shoe(6), reinforment_long, .6)\n",
    "runner_l.run([[10,8,10]], [200], [.01], 1, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player purse:  1000  this round\n",
      "Player bet:  20.0  this round\n",
      "Player initial hand:  ['3', '9']\n",
      "Dealer initial hand:  ['8', 'Jack']\n",
      "Player hand:  ['3', '9', '10']\n",
      "Player busts\n",
      "Final dealer hand:  ['8', 'Jack']\n",
      "980.0\n",
      "Player purse:  980.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  980.0  this round\n",
      "Player bet:  980.0  this round\n",
      "Player initial hand:  ['Queen', 'Jack']\n",
      "Dealer initial hand:  ['8', '5']\n",
      "Player hand:  ['Queen', 'Jack', 'Ace']\n",
      "Player hand:  ['Queen', 'Jack', 'Ace', 'Jack']\n",
      "Player busts\n",
      "Final dealer hand:  ['8', '5', 'King']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  320.0  this round\n",
      "Player initial hand:  ['10', 'Ace']\n",
      "Dealer initial hand:  ['10', 'Jack']\n",
      "Player stands\n",
      "Final dealer hand:  ['10', 'Jack']\n",
      "1320.0\n",
      "Player purse:  1320.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  1320.0  this round\n",
      "Player bet:  1280.0  this round\n",
      "Player initial hand:  ['King', 'Ace']\n",
      "Dealer initial hand:  ['Queen', 'Ace']\n",
      "Player hand:  ['King', 'Ace', 'Jack']\n",
      "Player stands\n",
      "Final dealer hand:  ['Queen', 'Ace']\n",
      "40.0\n",
      "Player purse:  40.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  40.0  this round\n",
      "Player bet:  40.0  this round\n",
      "Player initial hand:  ['5', 'Jack']\n",
      "Dealer initial hand:  ['9', '4']\n",
      "Player stands\n",
      "Final dealer hand:  ['9', '4', 'Ace', '8']\n",
      "80.0\n",
      "Player purse:  80.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  80.0  this round\n",
      "Player bet:  80.0  this round\n",
      "Player initial hand:  ['2', '9']\n",
      "Dealer initial hand:  ['Ace', '8']\n",
      "Player stands\n",
      "Final dealer hand:  ['Ace', '8']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  1000.0  this round\n",
      "Player initial hand:  ['Queen', '3']\n",
      "Dealer initial hand:  ['9', 'Ace']\n",
      "Player hand:  ['Queen', '3', '6']\n",
      "Player hand:  ['Queen', '3', '6', '9']\n",
      "Player busts\n",
      "Final dealer hand:  ['9', 'Ace']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  20.0  this round\n",
      "Player initial hand:  ['King', '2']\n",
      "Dealer initial hand:  ['4', 'King']\n",
      "Player hand:  ['King', '2', '3']\n",
      "Player hand:  ['King', '2', '3', '9']\n",
      "Player busts\n",
      "Final dealer hand:  ['4', 'King', '2', '8']\n",
      "980.0\n",
      "Player purse:  980.0  after round\n",
      "--------------------------------------\n",
      "Player purse:  980.0  this round\n",
      "Player bet:  980.0  this round\n",
      "Player initial hand:  ['King', '2']\n",
      "Dealer initial hand:  ['5', '9']\n",
      "Player hand:  ['King', '2', 'Ace']\n",
      "Player hand:  ['King', '2', 'Ace', '6']\n",
      "Player hand:  ['King', '2', 'Ace', '6', '2']\n",
      "Player hand:  ['King', '2', 'Ace', '6', '2', '9']\n",
      "Player busts\n",
      "Final dealer hand:  ['5', '9', '3']\n",
      "0.0\n",
      "Player purse:  0.0  after round\n",
      "--------------------------------------\n",
      "Player is broke\n",
      "Player purse:  1000  this round\n",
      "Player bet:  320.0  this round\n",
      "Player initial hand:  ['4', '6']\n",
      "Dealer initial hand:  ['6', '9']\n",
      "Player stands\n",
      "Final dealer hand:  ['6', '9', '5']\n",
      "680.0\n",
      "Player purse:  680.0  after round\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "long_qnet = runner_l.Qnet\n",
    "for _ in range(10):\n",
    "    play_betting(long_qnet, runner_l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the long term reinforcement does actually ease the overbetting problem.\n",
    "This fits with the hypothesis about why the short term reinforcement was overzealous.\n",
    "The long term reinforcement has the opprotunity to see how actions in one hand affects its outcome down the road.\n",
    "Even still the agent does make large questionable bets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have analyzed reinforcement learning for the game of blackjack with various approaches.\n",
    "In general, none of the agents were able to play in a way that would be advisable to adopt in a casino.\n",
    "Several factors likely contributed to this.\n",
    "The input space is rather various, and many games will need to be played for the net to see every state.\n",
    "Beyond that the random nature of the game would necessitate that the agents see these states numerous times to truly assess the situation presented by each state.\n",
    "I expect that with significantly more resources for training, the networks precented here would see a marked increase in performance.\n",
    "\n",
    "Further fine tuning could come in several considerations as well.\n",
    "With more resources, more extensive tuning of hyperparameters could be done.\n",
    "Further, the networks have been shown to be very sensitive to the reward function.\n",
    "Making more thoughtful reward functions could also lead to more profitable performance.\n",
    "\n",
    "Despite the apparent limitations of this approach, it is clear that the agents were learning something.\n",
    "The strategies they adopted range from over aggressive to over conservative, but sometimes it made reasonable decisions.\n",
    "\n",
    "The effects of dimensionality reduction were also considered.\n",
    "These turned out to perform poorly and vastly increase training time.\n",
    "The autoecoders used were not perfect and could easily be improved, as they could be made to converge with complete information.\n",
    "While the use of CNN's was not explored here in practice, the consideration of hypothetical architectures that was presented could serve as a future area of improvement.\n",
    "\n",
    "Finally, while the goal of comparing the networks to traditional card counting methods was not acheived, because of the poor performance of the agents this comparison did not seem necessary. Watching the gameplay of the agents clearly demonstrated the shortcomings in their strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
